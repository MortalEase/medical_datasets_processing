#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
COCOæ•°æ®é›†åˆ’åˆ†å·¥å…·
å°†COCOæ ¼å¼æ•°æ®é›†æŒ‰æŒ‡å®šæ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†

åŠŸèƒ½ç‰¹ç‚¹:
1. æ”¯æŒCOCOæ ¼å¼æ•°æ®é›†çš„æ™ºèƒ½åˆ’åˆ†
2. ç¡®ä¿å„ç±»åˆ«åœ¨å„æ•°æ®é›†ä¸­çš„åˆ†å¸ƒå‡è¡¡
3. ä¿æŒå›¾åƒå’Œæ ‡æ³¨çš„å¯¹åº”å…³ç³»
4. æ”¯æŒè‡ªå®šä¹‰åˆ’åˆ†æ¯”ä¾‹
5. ç”Ÿæˆåˆ’åˆ†åçš„COCOæ ¼å¼æ ‡æ³¨æ–‡ä»¶

ä½œè€…: Medical Dataset Processing Team
æ—¥æœŸ: 2025-07-24
"""

import os
import json
import shutil
import random
import argparse
from collections import defaultdict, Counter
from pathlib import Path
import numpy as np


def get_image_extensions():
    """è¿”å›æ”¯æŒçš„å›¾ç‰‡æ ¼å¼æ‰©å±•å"""
    return ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.webp']


def load_coco_annotations(annotation_file):
    """
    åŠ è½½COCOæ ¼å¼æ ‡æ³¨æ–‡ä»¶
    
    Args:
        annotation_file (str): COCOæ ‡æ³¨æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: COCOæ ¼å¼æ•°æ®
    """
    with open(annotation_file, 'r', encoding='utf-8') as f:
        coco_data = json.load(f)
    return coco_data


def analyze_dataset_distribution(coco_data):
    """
    åˆ†ææ•°æ®é›†çš„ç±»åˆ«åˆ†å¸ƒ
    
    Args:
        coco_data (dict): COCOæ ¼å¼æ•°æ®
        
    Returns:
        dict: åˆ†æç»“æœ
    """
    # ç»Ÿè®¡æ¯ä¸ªå›¾åƒåŒ…å«çš„ç±»åˆ«
    image_to_categories = defaultdict(set)
    category_to_images = defaultdict(set)
    
    for annotation in coco_data['annotations']:
        image_id = annotation['image_id']
        category_id = annotation['category_id']
        
        image_to_categories[image_id].add(category_id)
        category_to_images[category_id].add(image_id)
    
    # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
    category_counts = Counter()
    for annotation in coco_data['annotations']:
        category_counts[annotation['category_id']] += 1
    
    return {
        'image_to_categories': dict(image_to_categories),
        'category_to_images': dict(category_to_images),
        'category_counts': category_counts,
        'total_images': len(coco_data['images']),
        'total_annotations': len(coco_data['annotations'])
    }


def stratified_split_images(coco_data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_state=42):
    """
    æŒ‰ç±»åˆ«åˆ†å±‚åˆ’åˆ†å›¾åƒï¼Œç¡®ä¿å„ç±»åˆ«åœ¨å„æ•°æ®é›†ä¸­çš„åˆ†å¸ƒå‡è¡¡
    
    Args:
        coco_data (dict): COCOæ ¼å¼æ•°æ®
        train_ratio (float): è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio (float): éªŒè¯é›†æ¯”ä¾‹  
        test_ratio (float): æµ‹è¯•é›†æ¯”ä¾‹
        random_state (int): éšæœºç§å­
        
    Returns:
        dict: åˆ’åˆ†ç»“æœ {'train': [], 'val': [], 'test': []}
    """
    random.seed(random_state)
    np.random.seed(random_state)
    
    # åˆ†ææ•°æ®é›†åˆ†å¸ƒ
    analysis = analyze_dataset_distribution(coco_data)
    image_to_categories = analysis['image_to_categories']
    category_to_images = analysis['category_to_images']
    
    # è·å–æ‰€æœ‰å›¾åƒID
    all_image_ids = [img['id'] for img in coco_data['images']]
    
    # å¦‚æœæ²¡æœ‰æ ‡æ³¨ï¼Œç®€å•éšæœºåˆ’åˆ†
    if not coco_data['annotations']:
        print("âš ï¸ æ•°æ®é›†ä¸­æ²¡æœ‰æ ‡æ³¨ï¼Œå°†è¿›è¡Œç®€å•éšæœºåˆ’åˆ†")
        random.shuffle(all_image_ids)
        
        train_count = int(len(all_image_ids) * train_ratio)
        val_count = int(len(all_image_ids) * val_ratio)
        
        return {
            'train': all_image_ids[:train_count],
            'val': all_image_ids[train_count:train_count + val_count],
            'test': all_image_ids[train_count + val_count:]
        }
    
    # æœ‰æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œåˆ†å±‚åˆ’åˆ†
    assigned_images = set()
    train_images, val_images, test_images = [], [], []
    
    # æŒ‰ç±»åˆ«é¢‘ç‡æ’åºï¼Œä¼˜å…ˆå¤„ç†æ ·æœ¬è¾ƒå°‘çš„ç±»åˆ«
    sorted_categories = sorted(category_to_images.keys(), 
                             key=lambda x: len(category_to_images[x]))
    
    for category_id in sorted_categories:
        category_images = list(category_to_images[category_id])
        # ç§»é™¤å·²åˆ†é…çš„å›¾åƒ
        category_images = [img_id for img_id in category_images if img_id not in assigned_images]
        
        if not category_images:
            continue
            
        random.shuffle(category_images)
        
        # è®¡ç®—è¯¥ç±»åˆ«çš„åˆ’åˆ†æ•°é‡
        total = len(category_images)
        train_count = max(1, int(total * train_ratio))
        val_count = max(0, int(total * val_ratio)) if total > 1 else 0
        test_count = total - train_count - val_count
        
        # åˆ†é…å›¾åƒ
        train_images.extend(category_images[:train_count])
        val_images.extend(category_images[train_count:train_count + val_count])
        test_images.extend(category_images[train_count + val_count:train_count + val_count + test_count])
        
        # æ ‡è®°ä¸ºå·²åˆ†é…
        assigned_images.update(category_images)
    
    # å¤„ç†æ²¡æœ‰æ ‡æ³¨çš„å›¾åƒï¼ˆèƒŒæ™¯å›¾åƒï¼‰
    unassigned_images = [img_id for img_id in all_image_ids if img_id not in assigned_images]
    if unassigned_images:
        print(f"å‘ç° {len(unassigned_images)} å¼ æ— æ ‡æ³¨å›¾åƒï¼Œè¿›è¡Œéšæœºåˆ†é…")
        random.shuffle(unassigned_images)
        
        bg_train_count = int(len(unassigned_images) * train_ratio)
        bg_val_count = int(len(unassigned_images) * val_ratio)
        
        train_images.extend(unassigned_images[:bg_train_count])
        val_images.extend(unassigned_images[bg_train_count:bg_train_count + bg_val_count])
        test_images.extend(unassigned_images[bg_train_count + bg_val_count:])
    
    return {
        'train': train_images,
        'val': val_images,
        'test': test_images
    }


def create_split_coco_data(coco_data, image_ids, split_name):
    """
    æ ¹æ®å›¾åƒIDåˆ—è¡¨åˆ›å»ºå¯¹åº”çš„COCOæ•°æ®å­é›†
    
    Args:
        coco_data (dict): åŸå§‹COCOæ•°æ®
        image_ids (list): å›¾åƒIDåˆ—è¡¨
        split_name (str): æ•°æ®é›†åˆ’åˆ†åç§°
        
    Returns:
        dict: åˆ’åˆ†åçš„COCOæ•°æ®
    """
    image_ids_set = set(image_ids)
    
    # è¿‡æ»¤å›¾åƒ
    split_images = [img for img in coco_data['images'] if img['id'] in image_ids_set]
    
    # è¿‡æ»¤æ ‡æ³¨
    split_annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] in image_ids_set]
    
    # é‡æ–°åˆ†é…IDï¼ˆå¯é€‰ï¼Œä¿æŒåŸIDä¹Ÿå¯ä»¥ï¼‰
    # è¿™é‡Œä¿æŒåŸIDä»¥ä¾¿äºè¿½è¸ª
    
    # åˆ›å»ºæ–°çš„COCOæ•°æ®ç»“æ„
    split_coco_data = {
        'info': coco_data.get('info', {}),
        'licenses': coco_data.get('licenses', []),
        'categories': coco_data['categories'],
        'images': split_images,
        'annotations': split_annotations
    }
    
    # æ›´æ–°infoä¸­çš„æè¿°
    if 'info' in split_coco_data:
        split_coco_data['info']['description'] = f"{split_coco_data['info'].get('description', '')} - {split_name} split"
    
    return split_coco_data


def copy_images(image_list, src_images_dir, dst_images_dir):
    """
    å¤åˆ¶å›¾åƒæ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•
    
    Args:
        image_list (list): å›¾åƒä¿¡æ¯åˆ—è¡¨
        src_images_dir (str): æºå›¾åƒç›®å½•
        dst_images_dir (str): ç›®æ ‡å›¾åƒç›®å½•
    """
    os.makedirs(dst_images_dir, exist_ok=True)
    
    copied_count = 0
    for image_info in image_list:
        src_path = os.path.join(src_images_dir, image_info['file_name'])
        dst_path = os.path.join(dst_images_dir, image_info['file_name'])
        
        if os.path.exists(src_path):
            shutil.copy2(src_path, dst_path)
            copied_count += 1
        else:
            print(f"âš ï¸ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {src_path}")
    
    print(f"âœ… å¤åˆ¶äº† {copied_count}/{len(image_list)} å¼ å›¾åƒ")


def print_split_statistics(splits, coco_data):
    """
    æ‰“å°åˆ’åˆ†ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        splits (dict): åˆ’åˆ†ç»“æœ
        coco_data (dict): åŸå§‹COCOæ•°æ®
    """
    analysis = analyze_dataset_distribution(coco_data)
    
    print("\n=== æ•°æ®é›†åˆ’åˆ†ç»Ÿè®¡ ===")
    print(f"åŸå§‹æ•°æ®é›†:")
    print(f"  - æ€»å›¾åƒæ•°: {analysis['total_images']}")
    print(f"  - æ€»æ ‡æ³¨æ•°: {analysis['total_annotations']}")
    print(f"  - ç±»åˆ«æ•°: {len(coco_data['categories'])}")
    
    for split_name, image_ids in splits.items():
        split_annotations = [ann for ann in coco_data['annotations'] 
                           if ann['image_id'] in image_ids]
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        category_counts = Counter(ann['category_id'] for ann in split_annotations)
        
        print(f"\n{split_name.upper()}é›†:")
        print(f"  - å›¾åƒæ•°: {len(image_ids)} ({len(image_ids)/analysis['total_images']*100:.1f}%)")
        print(f"  - æ ‡æ³¨æ•°: {len(split_annotations)}")
        
        if category_counts:
            print(f"  - ç±»åˆ«åˆ†å¸ƒ:")
            for cat in coco_data['categories']:
                count = category_counts.get(cat['id'], 0)
                print(f"    * {cat['name']}: {count}")


def split_coco_dataset(input_dir, output_dir, split_ratios, random_state=42):
    """
    åˆ’åˆ†COCOæ ¼å¼æ•°æ®é›†
    
    Args:
        input_dir (str): è¾“å…¥æ•°æ®é›†ç›®å½•ï¼ˆåŒ…å«imagesæ–‡ä»¶å¤¹å’Œannotations.jsonï¼‰
        output_dir (str): è¾“å‡ºæ•°æ®é›†ç›®å½•
        split_ratios (dict): åˆ’åˆ†æ¯”ä¾‹
        random_state (int): éšæœºç§å­
    """
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•ç»“æ„
    images_dir = input_path / 'images'
    annotation_file = input_path / 'annotations.json'
    
    if not images_dir.exists():
        raise FileNotFoundError(f"å›¾åƒç›®å½•ä¸å­˜åœ¨: {images_dir}")
    
    if not annotation_file.exists():
        raise FileNotFoundError(f"æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {annotation_file}")
    
    print(f"åŠ è½½COCOæ ‡æ³¨æ–‡ä»¶: {annotation_file}")
    coco_data = load_coco_annotations(annotation_file)
    
    print(f"åŸå§‹æ•°æ®é›†åŒ…å«:")
    print(f"  - å›¾åƒæ•°: {len(coco_data['images'])}")
    print(f"  - æ ‡æ³¨æ•°: {len(coco_data['annotations'])}")
    print(f"  - ç±»åˆ«æ•°: {len(coco_data['categories'])}")
    
    # æ‰§è¡Œåˆ’åˆ†
    print("\nå¼€å§‹æ‰§è¡Œæ•°æ®é›†åˆ’åˆ†...")
    splits = stratified_split_images(
        coco_data, 
        train_ratio=split_ratios['train'],
        val_ratio=split_ratios['val'],
        test_ratio=split_ratios['test'],
        random_state=random_state
    )
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print_split_statistics(splits, coco_data)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•å¹¶å¤åˆ¶æ–‡ä»¶
    print(f"\nåˆ›å»ºè¾“å‡ºç›®å½•: {output_path}")
    output_path.mkdir(parents=True, exist_ok=True)
    
    for split_name, image_ids in splits.items():
        if not image_ids:
            print(f"âš ï¸ {split_name}é›†ä¸ºç©ºï¼Œè·³è¿‡")
            continue
            
        print(f"\nå¤„ç† {split_name} æ•°æ®é›†...")
        
        # åˆ›å»ºåˆ’åˆ†ç›®å½•
        split_dir = output_path / split_name
        split_images_dir = split_dir / 'images'
        split_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºåˆ’åˆ†åçš„COCOæ•°æ®
        split_coco_data = create_split_coco_data(coco_data, image_ids, split_name)
        
        # ä¿å­˜æ ‡æ³¨æ–‡ä»¶
        split_annotation_file = split_dir / 'annotations.json'
        with open(split_annotation_file, 'w', encoding='utf-8') as f:
            json.dump(split_coco_data, f, indent=2, ensure_ascii=False)
        
        # å¤åˆ¶å›¾åƒæ–‡ä»¶
        copy_images(split_coco_data['images'], str(images_dir), str(split_images_dir))
        
        print(f"âœ… {split_name} æ•°æ®é›†å¤„ç†å®Œæˆ")
    
    # å¤åˆ¶é¢å¤–æ–‡ä»¶
    for extra_file in ['classes.txt', 'dataset_info.json']:
        src_file = input_path / extra_file
        if src_file.exists():
            dst_file = output_path / extra_file
            shutil.copy2(src_file, dst_file)
            print(f"âœ… å¤åˆ¶é¢å¤–æ–‡ä»¶: {extra_file}")
    
    print(f"\nğŸ‰ æ•°æ®é›†åˆ’åˆ†å®Œæˆ!")
    print(f"è¾“å‡ºç›®å½•: {output_path}")
    
    # æ˜¾ç¤ºæœ€ç»ˆç›®å½•ç»“æ„
    print("\næœ€ç»ˆç›®å½•ç»“æ„:")
    print(f"{output_path.name}/")
    for split_name in ['train', 'val', 'test']:
        split_dir = output_path / split_name
        if split_dir.exists():
            image_count = len(list((split_dir / 'images').glob('*'))) if (split_dir / 'images').exists() else 0
            print(f"â”œâ”€â”€ {split_name}/")
            print(f"â”‚   â”œâ”€â”€ images/ ({image_count} å¼ å›¾åƒ)")
            print(f"â”‚   â””â”€â”€ annotations.json")
    
    # åˆ—å‡ºé¢å¤–æ–‡ä»¶
    extra_files = [f.name for f in output_path.glob('*.txt') if f.is_file()] + \
                  [f.name for f in output_path.glob('*.json') if f.is_file() and f.name not in ['train', 'val', 'test']]
    
    if extra_files:
        for i, filename in enumerate(extra_files):
            prefix = "â””â”€â”€" if i == len(extra_files) - 1 else "â”œâ”€â”€"
            print(f"{prefix} {filename}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="COCOæ ¼å¼æ•°æ®é›†åˆ’åˆ†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python coco_dataset_split.py -i RibFrac-COCO-Full -o RibFrac-COCO-Split
  
  python coco_dataset_split.py -i RibFrac-COCO-Full -o RibFrac-COCO-Split \\
                               --train_ratio 0.8 --val_ratio 0.1 --test_ratio 0.1

è¾“å…¥ç›®å½•ç»“æ„:
  RibFrac-COCO-Full/
  â”œâ”€â”€ images/           # æ‰€æœ‰å›¾åƒæ–‡ä»¶
  â”œâ”€â”€ annotations.json  # COCOæ ¼å¼æ ‡æ³¨æ–‡ä»¶
  â”œâ”€â”€ classes.txt       # ç±»åˆ«æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
  â””â”€â”€ dataset_info.json # æ•°æ®é›†ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰

è¾“å‡ºç›®å½•ç»“æ„:
  RibFrac-COCO-Split/
  â”œâ”€â”€ train/
  â”‚   â”œâ”€â”€ images/
  â”‚   â””â”€â”€ annotations.json
  â”œâ”€â”€ val/
  â”‚   â”œâ”€â”€ images/
  â”‚   â””â”€â”€ annotations.json
  â”œâ”€â”€ test/
  â”‚   â”œâ”€â”€ images/
  â”‚   â””â”€â”€ annotations.json
  â”œâ”€â”€ classes.txt
  â””â”€â”€ dataset_info.json
        """
    )
    
    parser.add_argument('-i', '--input_dir', required=True,
                        help='è¾“å…¥COCOæ•°æ®é›†ç›®å½•ï¼ˆåŒ…å«imageså’Œannotations.jsonï¼‰')
    parser.add_argument('-o', '--output_dir', required=True,
                        help='è¾“å‡ºåˆ’åˆ†åæ•°æ®é›†ç›®å½•')
    parser.add_argument('--train_ratio', type=float, default=0.8,
                        help='è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤: 0.8)')
    parser.add_argument('--val_ratio', type=float, default=0.1,
                        help='éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤: 0.1)')
    parser.add_argument('--test_ratio', type=float, default=0.1,
                        help='æµ‹è¯•é›†æ¯”ä¾‹ (é»˜è®¤: 0.1)')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­ (é»˜è®¤: 42)')
    
    args = parser.parse_args()
    
    # éªŒè¯æ¯”ä¾‹æ€»å’Œ
    total_ratio = args.train_ratio + args.val_ratio + args.test_ratio
    if abs(total_ratio - 1.0) > 1e-6:
        print(f"âŒ é”™è¯¯: è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†æ¯”ä¾‹æ€»å’Œåº”ä¸º1.0ï¼Œå½“å‰ä¸º{total_ratio}")
        return
    
    # éªŒè¯è¾“å…¥ç›®å½•
    if not os.path.exists(args.input_dir):
        print(f"âŒ é”™è¯¯: è¾“å…¥ç›®å½• {args.input_dir} ä¸å­˜åœ¨")
        return
    
    # æ„å»ºæ¯”ä¾‹å­—å…¸
    split_ratios = {
        'train': args.train_ratio,
        'val': args.val_ratio,
        'test': args.test_ratio
    }
    
    print(f"=== COCOæ•°æ®é›†åˆ’åˆ†å·¥å…· ===")
    print(f"è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"è®­ç»ƒé›†æ¯”ä¾‹: {args.train_ratio}")
    print(f"éªŒè¯é›†æ¯”ä¾‹: {args.val_ratio}")
    print(f"æµ‹è¯•é›†æ¯”ä¾‹: {args.test_ratio}")
    print(f"éšæœºç§å­: {args.seed}")
    print("-" * 50)
    
    try:
        # æ‰§è¡Œæ•°æ®é›†åˆ’åˆ†
        split_coco_dataset(
            input_dir=args.input_dir,
            output_dir=args.output_dir,
            split_ratios=split_ratios,
            random_state=args.seed
        )
        
    except Exception as e:
        print(f"âŒ åˆ’åˆ†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
