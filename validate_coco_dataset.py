#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
COCOæ ¼å¼æ•°æ®é›†éªŒè¯è„šæœ¬
ç”¨äºéªŒè¯COCOæ ¼å¼æ•°æ®é›†çš„æ ‡ç­¾å’Œå›¾åƒå¯¹åº”å…³ç³»
"""

import os
import json
import argparse
from pathlib import Path
from collections import Counter

def validate_coco_dataset(dataset_path):
    """éªŒè¯COCOæ ¼å¼æ•°æ®é›†"""
    print(f"éªŒè¯COCOæ•°æ®é›†: {dataset_path}")
    
    # æŸ¥æ‰¾annotationsç›®å½•
    annotations_dir = os.path.join(dataset_path, 'annotations')
    if not os.path.exists(annotations_dir):
        print(f"âŒ æœªæ‰¾åˆ°annotationsç›®å½•: {annotations_dir}")
        return False
    
    # æŸ¥æ‰¾æ‰€æœ‰JSONæ ‡æ³¨æ–‡ä»¶
    json_files = list(Path(annotations_dir).glob('*.json'))
    if not json_files:
        print(f"âŒ annotationsç›®å½•ä¸­æœªæ‰¾åˆ°JSONæ–‡ä»¶")
        return False
    
    print(f"âœ… æ‰¾åˆ°{len(json_files)}ä¸ªæ ‡æ³¨æ–‡ä»¶")
    
    total_images = 0
    total_annotations = 0
    all_categories = set()
    
    for json_file in json_files:
        print(f"\nğŸ“‹ éªŒè¯æ ‡æ³¨æ–‡ä»¶: {json_file.name}")
        
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
        except Exception as e:
            print(f"âŒ è¯»å–JSONæ–‡ä»¶å¤±è´¥: {e}")
            continue
        
        # åŸºæœ¬ä¿¡æ¯
        images = data.get('images', [])
        annotations = data.get('annotations', [])
        categories = data.get('categories', [])
        
        print(f"  ğŸ“Š å›¾åƒæ•°é‡: {len(images)}")
        print(f"  ğŸ“Š æ ‡æ³¨æ•°é‡: {len(annotations)}")
        print(f"  ğŸ“Š ç±»åˆ«æ•°é‡: {len(categories)}")
        
        # ç´¯è®¡ç»Ÿè®¡
        total_images += len(images)
        total_annotations += len(annotations)
        
        # ç±»åˆ«ä¿¡æ¯
        for cat in categories:
            cat_info = f"ID{cat['id']}:{cat['name']}"
            all_categories.add(cat_info)
            print(f"    ç±»åˆ« {cat['id']}: {cat['name']}")
          # éªŒè¯å¯¹åº”çš„å›¾åƒç›®å½•
        # æ¨æ–­å›¾åƒç›®å½•å (å»æ‰instances_å‰ç¼€å’Œ.jsonåç¼€)
        img_dir_name = json_file.stem.replace('instances_', '')
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯fullï¼Œå¯¹åº”full2017ç›®å½•
        if img_dir_name == 'full':
            img_dir_name = 'full2017'
        
        img_dir_path = os.path.join(dataset_path, img_dir_name)
        
        if os.path.exists(img_dir_path):
            print(f"  âœ… æ‰¾åˆ°å¯¹åº”å›¾åƒç›®å½•: {img_dir_name}")
            
            # ç»Ÿè®¡å›¾åƒæ–‡ä»¶
            image_files = list(Path(img_dir_path).glob('*.png')) + \
                         list(Path(img_dir_path).glob('*.jpg')) + \
                         list(Path(img_dir_path).glob('*.jpeg'))
            
            print(f"  ğŸ“ å®é™…å›¾åƒæ–‡ä»¶æ•°: {len(image_files)}")
            
            # æ£€æŸ¥å›¾åƒ-æ ‡æ³¨å¯¹åº”å…³ç³»
            json_image_names = {img['file_name'] for img in images}
            actual_image_names = {img_file.name for img_file in image_files}
            
            missing_images = json_image_names - actual_image_names
            extra_images = actual_image_names - json_image_names
            
            if missing_images:
                print(f"  âš ï¸  æ ‡æ³¨ä¸­æœ‰{len(missing_images)}ä¸ªå›¾åƒåœ¨ç›®å½•ä¸­ä¸å­˜åœ¨")
                for img in list(missing_images)[:3]:
                    print(f"    ç¼ºå¤±: {img}")
                if len(missing_images) > 3:
                    print(f"    ...è¿˜æœ‰{len(missing_images)-3}ä¸ª")
            
            if extra_images:
                print(f"  âš ï¸  ç›®å½•ä¸­æœ‰{len(extra_images)}ä¸ªå›¾åƒæœªåœ¨æ ‡æ³¨ä¸­")
                for img in list(extra_images)[:3]:
                    print(f"    å¤šä½™: {img}")
                if len(extra_images) > 3:
                    print(f"    ...è¿˜æœ‰{len(extra_images)-3}ä¸ª")
            
            if not missing_images and not extra_images:
                print(f"  âœ… å›¾åƒ-æ ‡æ³¨å®Œå…¨å¯¹åº”")
        
        else:
            print(f"  âŒ æœªæ‰¾åˆ°å¯¹åº”å›¾åƒç›®å½•: {img_dir_name}")
        
        # æ ‡æ³¨ç»Ÿè®¡
        if annotations:
            category_counts = Counter(ann['category_id'] for ann in annotations)
            areas = [ann['area'] for ann in annotations if 'area' in ann]
            
            print(f"  ğŸ“ˆ æ ‡æ³¨åˆ†å¸ƒ:")
            for cat_id, count in category_counts.items():
                cat_name = next((cat['name'] for cat in categories if cat['id'] == cat_id), f"æœªçŸ¥_{cat_id}")
                percentage = count / len(annotations) * 100
                print(f"    ç±»åˆ« {cat_id} ({cat_name}): {count} ({percentage:.1f}%)")
            
            if areas:
                print(f"  ğŸ“¦ è¾¹ç•Œæ¡†é¢ç§¯: æœ€å°{min(areas):.1f}, æœ€å¤§{max(areas):.1f}, å¹³å‡{sum(areas)/len(areas):.1f}")
    
    # æ€»ä½“ç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ æ€»ä½“ç»Ÿè®¡:")
    print(f"  æ€»å›¾åƒæ•°: {total_images}")
    print(f"  æ€»æ ‡æ³¨æ•°: {total_annotations}")
    print(f"  æ‰€æœ‰ç±»åˆ«: {len(all_categories)}")
    for cat in sorted(all_categories):
        print(f"    {cat}")
    print(f"{'='*60}")
    
    return True

def main():
    parser = argparse.ArgumentParser(description="éªŒè¯COCOæ ¼å¼æ•°æ®é›†")
    parser.add_argument("-d", "--dataset", required=True, help="COCOæ•°æ®é›†æ ¹ç›®å½•è·¯å¾„")
    
    args = parser.parse_args()
    dataset_path = args.dataset
    
    print("=" * 70)
    print("          COCOæ ¼å¼æ•°æ®é›†éªŒè¯å·¥å…·")
    print("=" * 70)
    
    validate_coco_dataset(dataset_path)

if __name__ == "__main__":
    main()
