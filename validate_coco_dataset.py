#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
COCOæ ¼å¼æ•°æ®é›†éªŒè¯è„šæœ¬
ç”¨äºéªŒè¯COCOæ ¼å¼æ•°æ®é›†çš„æ ‡ç­¾å’Œå›¾åƒå¯¹åº”å…³ç³»
"""

import os
import json
import argparse
from pathlib import Path
from collections import Counter

def validate_coco_dataset(dataset_path):
    """éªŒè¯COCOæ ¼å¼æ•°æ®é›†"""
    print(f"éªŒè¯COCOæ•°æ®é›†: {dataset_path}")
    
    dataset_path = Path(dataset_path)
    
    # æ£€æŸ¥ä¸¤ç§å¯èƒ½çš„æ•°æ®é›†ç»“æ„
    # ç»“æ„1: æ–°æ ¼å¼ - train/val/test å­ç›®å½•ï¼Œæ¯ä¸ªåŒ…å« images/ å’Œ annotations.json
    # ç»“æ„2: ä¼ ç»Ÿæ ¼å¼ - annotations/ ç›®å½•åŒ…å«å¤šä¸ªJSONæ–‡ä»¶
    
    # å…ˆæ£€æŸ¥æ–°æ ¼å¼
    splits = ['train', 'val', 'test']
    new_format_splits = []
    
    for split in splits:
        split_dir = dataset_path / split
        if split_dir.exists():
            annotation_file = split_dir / 'annotations.json'
            images_dir = split_dir / 'images'
            if annotation_file.exists() and images_dir.exists():
                new_format_splits.append(split)
    
    if new_format_splits:
        print(f"âœ… æ£€æµ‹åˆ°æ–°æ ¼å¼COCOæ•°æ®é›†ï¼Œæ‰¾åˆ°{len(new_format_splits)}ä¸ªæ•°æ®åˆ’åˆ†")
        return validate_coco_new_format(dataset_path, new_format_splits)
    
    # æ£€æŸ¥ä¼ ç»Ÿæ ¼å¼
    annotations_dir = dataset_path / 'annotations'
    if annotations_dir.exists():
        json_files = list(annotations_dir.glob('*.json'))
        if json_files:
            print(f"âœ… æ£€æµ‹åˆ°ä¼ ç»Ÿæ ¼å¼COCOæ•°æ®é›†ï¼Œæ‰¾åˆ°{len(json_files)}ä¸ªæ ‡æ³¨æ–‡ä»¶")
            return validate_coco_traditional_format(dataset_path, json_files)
    
    print(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„COCOæ•°æ®é›†ç»“æ„")
    print(f"æœŸæœ›ç»“æ„1 (æ–°æ ¼å¼):")
    print(f"  {dataset_path}/train/annotations.json")
    print(f"  {dataset_path}/train/images/")
    print(f"æœŸæœ›ç»“æ„2 (ä¼ ç»Ÿæ ¼å¼):")
    print(f"  {dataset_path}/annotations/*.json")
    return False


def validate_coco_new_format(dataset_path, splits):
    """éªŒè¯æ–°æ ¼å¼COCOæ•°æ®é›† (train/val/testç»“æ„)"""
    
    total_images = 0
    total_annotations = 0
    all_categories = set()
    
    for split in splits:
        split_dir = dataset_path / split
        annotation_file = split_dir / 'annotations.json'
        images_dir = split_dir / 'images'
        
        print(f"\nğŸ“‹ éªŒè¯æ•°æ®åˆ’åˆ†: {split}")
        
        try:
            with open(annotation_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"âŒ è¯»å–æ ‡æ³¨æ–‡ä»¶å¤±è´¥: {e}")
            continue
        
        # åŸºæœ¬ä¿¡æ¯
        images = data.get('images', [])
        annotations = data.get('annotations', [])
        categories = data.get('categories', [])
        
        print(f"  ğŸ“Š å›¾åƒæ•°é‡: {len(images)}")
        print(f"  ğŸ“Š æ ‡æ³¨æ•°é‡: {len(annotations)}")
        print(f"  ğŸ“Š ç±»åˆ«æ•°é‡: {len(categories)}")
        
        # ç´¯è®¡ç»Ÿè®¡
        total_images += len(images)
        total_annotations += len(annotations)
        
        # ç±»åˆ«ä¿¡æ¯
        for cat in categories:
            cat_info = f"ID{cat['id']}:{cat['name']}"
            all_categories.add(cat_info)
            print(f"    ç±»åˆ« {cat['id']}: {cat['name']}")
        
        # éªŒè¯å›¾åƒç›®å½•
        if images_dir.exists():
            print(f"  âœ… æ‰¾åˆ°å¯¹åº”å›¾åƒç›®å½•: {images_dir.name}")
            
            # ç»Ÿè®¡å›¾åƒæ–‡ä»¶
            image_files = list(images_dir.glob('*.png')) + \
                         list(images_dir.glob('*.jpg')) + \
                         list(images_dir.glob('*.jpeg'))
            
            print(f"  ğŸ“ å®é™…å›¾åƒæ–‡ä»¶æ•°: {len(image_files)}")
            
            # æ£€æŸ¥å›¾åƒ-æ ‡æ³¨å¯¹åº”å…³ç³»
            json_image_names = {img['file_name'] for img in images}
            actual_image_names = {img_file.name for img_file in image_files}
            
            missing_images = json_image_names - actual_image_names
            extra_images = actual_image_names - json_image_names
            
            if missing_images:
                print(f"  âš ï¸  æ ‡æ³¨ä¸­æœ‰{len(missing_images)}ä¸ªå›¾åƒåœ¨ç›®å½•ä¸­ä¸å­˜åœ¨")
                for img in list(missing_images)[:3]:
                    print(f"    ç¼ºå¤±: {img}")
                if len(missing_images) > 3:
                    print(f"    ...è¿˜æœ‰{len(missing_images)-3}ä¸ª")
            
            if extra_images:
                print(f"  âš ï¸  ç›®å½•ä¸­æœ‰{len(extra_images)}ä¸ªå›¾åƒæœªåœ¨æ ‡æ³¨ä¸­")
                for img in list(extra_images)[:3]:
                    print(f"    å¤šä½™: {img}")
                if len(extra_images) > 3:
                    print(f"    ...è¿˜æœ‰{len(extra_images)-3}ä¸ª")
            
            if not missing_images and not extra_images:
                print(f"  âœ… å›¾åƒ-æ ‡æ³¨å®Œå…¨å¯¹åº”")
        else:
            print(f"  âŒ æœªæ‰¾åˆ°å¯¹åº”å›¾åƒç›®å½•: {images_dir}")
        
        # æ ‡æ³¨ç»Ÿè®¡
        if annotations:
            category_counts = Counter(ann['category_id'] for ann in annotations)
            areas = [ann['area'] for ann in annotations if 'area' in ann]
            
            print(f"  ğŸ“ˆ æ ‡æ³¨åˆ†å¸ƒ:")
            for cat_id, count in category_counts.items():
                cat_name = next((cat['name'] for cat in categories if cat['id'] == cat_id), f"æœªçŸ¥_{cat_id}")
                percentage = count / len(annotations) * 100
                print(f"    ç±»åˆ« {cat_id} ({cat_name}): {count} ({percentage:.1f}%)")
            
            if areas:
                print(f"  ğŸ“¦ è¾¹ç•Œæ¡†é¢ç§¯: æœ€å°{min(areas):.1f}, æœ€å¤§{max(areas):.1f}, å¹³å‡{sum(areas)/len(areas):.1f}")
    
    # æ£€æŸ¥é¢å¤–æ–‡ä»¶
    print(f"\nğŸ“„ æ£€æŸ¥é¢å¤–æ–‡ä»¶:")
    classes_file = dataset_path / 'classes.txt'
    info_file = dataset_path / 'dataset_info.json'
    
    if classes_file.exists():
        print(f"  âœ… æ‰¾åˆ°ç±»åˆ«æ–‡ä»¶: classes.txt")
        try:
            with open(classes_file, 'r', encoding='utf-8') as f:
                class_names = [line.strip() for line in f if line.strip()]
            print(f"    åŒ…å« {len(class_names)} ä¸ªç±»åˆ«: {', '.join(class_names)}")
        except Exception as e:
            print(f"    âš ï¸ è¯»å–å¤±è´¥: {e}")
    else:
        print(f"  âš ï¸ æœªæ‰¾åˆ°ç±»åˆ«æ–‡ä»¶: classes.txt")
    
    if info_file.exists():
        print(f"  âœ… æ‰¾åˆ°æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶: dataset_info.json")
        try:
            with open(info_file, 'r', encoding='utf-8') as f:
                info_data = json.load(f)
            print(f"    æ•°æ®é›†åç§°: {info_data.get('name', 'æœªçŸ¥')}")
            print(f"    æè¿°: {info_data.get('description', 'æ— ')}")
        except Exception as e:
            print(f"    âš ï¸ è¯»å–å¤±è´¥: {e}")
    else:
        print(f"  âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶: dataset_info.json")
    
    # æ€»ä½“ç»Ÿè®¡
    print_summary(total_images, total_annotations, all_categories)
    return True


def validate_coco_traditional_format(dataset_path, json_files):
    """éªŒè¯ä¼ ç»Ÿæ ¼å¼COCOæ•°æ®é›† (annotationsç›®å½•ç»“æ„)"""
    
    total_images = 0
    total_annotations = 0
    all_categories = set()
    
    for json_file in json_files:
        print(f"\nğŸ“‹ éªŒè¯æ ‡æ³¨æ–‡ä»¶: {json_file.name}")
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"âŒ è¯»å–JSONæ–‡ä»¶å¤±è´¥: {e}")
            continue
        
        # åŸºæœ¬ä¿¡æ¯
        images = data.get('images', [])
        annotations = data.get('annotations', [])
        categories = data.get('categories', [])
        
        print(f"  ğŸ“Š å›¾åƒæ•°é‡: {len(images)}")
        print(f"  ğŸ“Š æ ‡æ³¨æ•°é‡: {len(annotations)}")
        print(f"  ğŸ“Š ç±»åˆ«æ•°é‡: {len(categories)}")
        
        # ç´¯è®¡ç»Ÿè®¡
        total_images += len(images)
        total_annotations += len(annotations)
        
        # ç±»åˆ«ä¿¡æ¯
        for cat in categories:
            cat_info = f"ID{cat['id']}:{cat['name']}"
            all_categories.add(cat_info)
            print(f"    ç±»åˆ« {cat['id']}: {cat['name']}")
        
        # éªŒè¯å¯¹åº”çš„å›¾åƒç›®å½•
        # æ¨æ–­å›¾åƒç›®å½•å (å»æ‰instances_å‰ç¼€å’Œ.jsonåç¼€)
        img_dir_name = json_file.stem.replace('instances_', '')
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯fullï¼Œå¯¹åº”full2017ç›®å½•
        if img_dir_name == 'full':
            img_dir_name = 'full2017'
        
        img_dir_path = dataset_path / img_dir_name
        
        if img_dir_path.exists():
            print(f"  âœ… æ‰¾åˆ°å¯¹åº”å›¾åƒç›®å½•: {img_dir_name}")
            
            # ç»Ÿè®¡å›¾åƒæ–‡ä»¶
            image_files = list(img_dir_path.glob('*.png')) + \
                         list(img_dir_path.glob('*.jpg')) + \
                         list(img_dir_path.glob('*.jpeg'))
            
            print(f"  ğŸ“ å®é™…å›¾åƒæ–‡ä»¶æ•°: {len(image_files)}")
            
            # æ£€æŸ¥å›¾åƒ-æ ‡æ³¨å¯¹åº”å…³ç³»
            json_image_names = {img['file_name'] for img in images}
            actual_image_names = {img_file.name for img_file in image_files}
            
            missing_images = json_image_names - actual_image_names
            extra_images = actual_image_names - json_image_names
            
            if missing_images:
                print(f"  âš ï¸  æ ‡æ³¨ä¸­æœ‰{len(missing_images)}ä¸ªå›¾åƒåœ¨ç›®å½•ä¸­ä¸å­˜åœ¨")
                for img in list(missing_images)[:3]:
                    print(f"    ç¼ºå¤±: {img}")
                if len(missing_images) > 3:
                    print(f"    ...è¿˜æœ‰{len(missing_images)-3}ä¸ª")
            
            if extra_images:
                print(f"  âš ï¸  ç›®å½•ä¸­æœ‰{len(extra_images)}ä¸ªå›¾åƒæœªåœ¨æ ‡æ³¨ä¸­")
                for img in list(extra_images)[:3]:
                    print(f"    å¤šä½™: {img}")
                if len(extra_images) > 3:
                    print(f"    ...è¿˜æœ‰{len(extra_images)-3}ä¸ª")
            
            if not missing_images and not extra_images:
                print(f"  âœ… å›¾åƒ-æ ‡æ³¨å®Œå…¨å¯¹åº”")
        
        else:
            print(f"  âŒ æœªæ‰¾åˆ°å¯¹åº”å›¾åƒç›®å½•: {img_dir_name}")
        
        # æ ‡æ³¨ç»Ÿè®¡
        if annotations:
            category_counts = Counter(ann['category_id'] for ann in annotations)
            areas = [ann['area'] for ann in annotations if 'area' in ann]
            
            print(f"  ğŸ“ˆ æ ‡æ³¨åˆ†å¸ƒ:")
            for cat_id, count in category_counts.items():
                cat_name = next((cat['name'] for cat in categories if cat['id'] == cat_id), f"æœªçŸ¥_{cat_id}")
                percentage = count / len(annotations) * 100
                print(f"    ç±»åˆ« {cat_id} ({cat_name}): {count} ({percentage:.1f}%)")
            
            if areas:
                print(f"  ğŸ“¦ è¾¹ç•Œæ¡†é¢ç§¯: æœ€å°{min(areas):.1f}, æœ€å¤§{max(areas):.1f}, å¹³å‡{sum(areas)/len(areas):.1f}")
    
    # æ€»ä½“ç»Ÿè®¡
    print_summary(total_images, total_annotations, all_categories)
    return True


def print_summary(total_images, total_annotations, all_categories):
    """æ‰“å°æ€»ä½“ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ æ€»ä½“ç»Ÿè®¡:")
    print(f"  æ€»å›¾åƒæ•°: {total_images}")
    print(f"  æ€»æ ‡æ³¨æ•°: {total_annotations}")
    print(f"  æ‰€æœ‰ç±»åˆ«: {len(all_categories)}")
    for cat in sorted(all_categories):
        print(f"    {cat}")
    print(f"{'='*60}")

def main():
    parser = argparse.ArgumentParser(description="éªŒè¯COCOæ ¼å¼æ•°æ®é›†")
    parser.add_argument("-d", "--dataset", required=True, help="COCOæ•°æ®é›†æ ¹ç›®å½•è·¯å¾„")
    
    args = parser.parse_args()
    dataset_path = args.dataset
    
    print("=" * 70)
    print("          COCOæ ¼å¼æ•°æ®é›†éªŒè¯å·¥å…·")
    print("=" * 70)
    
    validate_coco_dataset(dataset_path)

if __name__ == "__main__":
    main()
