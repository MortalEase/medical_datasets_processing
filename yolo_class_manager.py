import os
import shutil
import argparse
from collections import defaultdict
from datetime import datetime
from utils.logging_utils import tee_stdout_stderr
_LOG_FILE = tee_stdout_stderr('logs')
from utils.yolo_utils import (
    detect_yolo_structure,
    yolo_label_dirs,
    iter_label_files,
    list_possible_class_files,
    read_class_names,
    write_class_names,
    get_folder_size,
)




def analyze_dataset_classes(base_dir):
    """åˆ†ææ•°æ®é›†ä¸­çš„ç±»åˆ«ä½¿ç”¨æƒ…å†µ
    Returns: (class_usage: dict[int,int], class_names: list[str])
    """
    structure, _, _ = detect_yolo_structure(base_dir)
    
    if structure == 'unknown':
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®é›†ç»“æ„")
        return None, None
    
    print(f"ğŸ“ æ£€æµ‹åˆ°æ•°æ®é›†ç»“æ„: {structure}")
    
    # è·å–æ‰€æœ‰æ ‡ç­¾ç›®å½•
    label_dirs = yolo_label_dirs(base_dir, structure)
    
    # ç»Ÿè®¡ç±»åˆ«ä½¿ç”¨æƒ…å†µ
    class_usage = defaultdict(int)  # {class_id: count}
    total_annotations = 0
    
    for labels_dir in label_dirs:
        for label_file in iter_label_files(labels_dir, structure):
            label_path = os.path.join(labels_dir, label_file)
            try:
                with open(label_path, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            class_id = int(line.split()[0])
                            class_usage[class_id] += 1
                            total_annotations += 1
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•è¯»å–æ ‡ç­¾æ–‡ä»¶ {label_path}: {e}")
    
    # æŸ¥æ‰¾ç±»åˆ«æ–‡ä»¶
    class_files = list_possible_class_files(base_dir)
    class_names = []
    
    if class_files:
        class_file_path = os.path.join(base_dir, class_files[0])
        class_names = read_class_names(class_file_path)
        print(f"ğŸ“‹ æ‰¾åˆ°ç±»åˆ«æ–‡ä»¶: {class_files[0]}")
    
    return class_usage, class_names


def delete_classes(base_dir, explicit_class_ids=None, backup=True, min_samples=None, min_percentage=None, assume_yes=False, dry_run=False):
    """åˆ é™¤ç±»åˆ«ï¼šæ”¯æŒæ˜¾å¼IDã€æœ€å°æ ·æœ¬æ•°é˜ˆå€¼ã€æœ€å°å æ¯”é˜ˆå€¼ã€‚"""
    explicit_class_ids = set(explicit_class_ids or [])
    class_usage, class_names = analyze_dataset_classes(base_dir)
    if class_usage is None:
        return False

    total = sum(class_usage.values()) or 1
    # è‡ªåŠ¨é˜ˆå€¼é€‰æ‹©
    auto_ids = set()
    if min_samples is not None:
        auto_ids.update([cid for cid, cnt in class_usage.items() if cnt < min_samples])
    if min_percentage is not None:
        min_cnt = total * (float(min_percentage) / 100.0)
        auto_ids.update([cid for cid, cnt in class_usage.items() if cnt < min_cnt])

    target_ids = explicit_class_ids | auto_ids
    if not target_ids:
        print("æœªæŒ‡å®šéœ€è¦åˆ é™¤çš„ç±»åˆ« (æ—¢æ²¡æœ‰IDä¹Ÿæ²¡æœ‰é˜ˆå€¼å‘½ä¸­)")
        return False

    all_defined_classes = set(range(len(class_names))) if class_names else set()
    invalid = target_ids - all_defined_classes
    if invalid:
        print(f"é”™è¯¯: ä»¥ä¸‹ç±»åˆ«è¶…å‡ºå®šä¹‰èŒƒå›´ (0-{len(class_names)-1}): {sorted(invalid)}")
        return False

    used_classes = set(class_usage.keys())
    used_to_delete = sorted(target_ids & used_classes)
    unused_to_delete = sorted(target_ids - used_classes)

    removed_ann = sum(class_usage.get(cid, 0) for cid in used_to_delete)
    preview_lines = ["====== åˆ é™¤é¢„è§ˆ ======",
                     f"æ€»æ ‡æ³¨: {total}",
                     f"æ‹Ÿåˆ é™¤ç±»åˆ«æ€»æ•°: {len(target_ids)} (å·²ä½¿ç”¨ {len(used_to_delete)}, æœªä½¿ç”¨ {len(unused_to_delete)})",
                     f"å·²ä½¿ç”¨åˆ é™¤ç±»åˆ«: {used_to_delete}",
                     f"æœªä½¿ç”¨åˆ é™¤ç±»åˆ«: {unused_to_delete}",
                     f"å°†åˆ é™¤æ ‡æ³¨æ•°: {removed_ann} ({removed_ann*100.0/total:.1f}%)"]
    if min_samples is not None:
        preview_lines.append(f"æŒ‰æœ€å°æ ·æœ¬æ•°é˜ˆå€¼(<{min_samples})åŒ¹é…: {sorted([c for c in class_usage if class_usage[c] < min_samples])}")
    if min_percentage is not None:
        preview_lines.append(f"æŒ‰æœ€å°å æ¯”é˜ˆå€¼(<{min_percentage}% )åŒ¹é…: {sorted([c for c in class_usage if class_usage[c] < total * (float(min_percentage)/100.0)])}")
    print('\n'.join(preview_lines))

    if dry_run:
        print("[é¢„è§ˆæ¨¡å¼] æœªå†™å…¥ã€‚ä½¿ç”¨ --execute è¿›è¡Œå®é™…åˆ é™¤ã€‚")
        return True

    if not assume_yes:
        ans = input("ç¡®è®¤æ‰§è¡Œåˆ é™¤? (y/n): ").strip().lower()
        if ans not in {'y', 'yes'}:
            print("å·²å–æ¶ˆ")
            return False

    # å¤‡ä»½
    if backup:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = f"{base_dir}_backup_before_delete_{timestamp}"
        if os.path.exists(backup_dir):
            shutil.rmtree(backup_dir)
        shutil.copytree(base_dir, backup_dir)
        print(f"âœ“ å·²åˆ›å»ºå¤‡ä»½: {backup_dir}")

    structure, _, _ = detect_yolo_structure(base_dir)
    label_dirs = yolo_label_dirs(base_dir, structure)

    remaining_used_classes = sorted([c for c in used_classes if c not in target_ids])
    class_mapping = {old: new for new, old in enumerate(remaining_used_classes)}

    deleted_annotations = 0
    updated_files = 0

    for labels_dir in label_dirs:
        for label_file in iter_label_files(labels_dir, structure):
            label_path = os.path.join(labels_dir, label_file)
            try:
                new_lines = []
                file_changed = False
                with open(label_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        s = line.strip()
                        if not s:
                            continue
                        parts = s.split()
                        try:
                            cid = int(parts[0])
                        except Exception:
                            continue
                        if cid in target_ids:
                            deleted_annotations += 1
                            file_changed = True
                            continue
                        # é‡æ–°æ˜ å°„
                        if cid in class_mapping and class_mapping[cid] != cid:
                            parts[0] = str(class_mapping[cid])
                            file_changed = True
                        new_lines.append(' '.join(parts))
                if file_changed:
                    with open(label_path, 'w', encoding='utf-8') as f:
                        for ln in new_lines:
                            f.write(ln + '\n')
                    updated_files += 1
            except Exception as e:
                print(f"é”™è¯¯: æ— æ³•å¤„ç†æ ‡ç­¾æ–‡ä»¶ {label_path}: {e}")

    # æ›´æ–°ç±»åˆ«æ–‡ä»¶
    class_files = list_possible_class_files(base_dir)
    if class_files and class_names:
        remaining_indices = [i for i in range(len(class_names)) if i not in target_ids]
        new_names = [class_names[i] for i in remaining_indices]
        for cf in class_files:
            write_class_names(os.path.join(base_dir, cf), new_names)
            print(f"âœ“ å·²æ›´æ–°ç±»åˆ«æ–‡ä»¶: {cf}")

    print("\nåˆ é™¤å®Œæˆ")
    print(f"åˆ é™¤ç±»åˆ«: {sorted(list(target_ids))}")
    print(f"åˆ é™¤æ ‡æ³¨: {deleted_annotations}")
    print(f"æ›´æ–°æ–‡ä»¶: {updated_files}")
    print(f"å‰©ä½™ç±»åˆ«: {len(class_names) - len(target_ids) if class_names else 0}")
    return True


def rename_classes(base_dir, class_renames, backup=True):
    """é‡å‘½åç±»åˆ« (åªæ›´æ–°ç±»åˆ«æ–‡ä»¶ä¸­çš„åç§°ï¼Œä¸æ”¹å˜æ ‡ç­¾æ–‡ä»¶ä¸­çš„ID)"""
    print(f"\nå¼€å§‹é‡å‘½åç±»åˆ«: {class_renames}")
    
    # æŸ¥æ‰¾ç±»åˆ«æ–‡ä»¶
    class_files = list_possible_class_files(base_dir)
    if not class_files:
        print("é”™è¯¯: æœªæ‰¾åˆ°ç±»åˆ«æ–‡ä»¶")
        return False
    
    # åˆ›å»ºå¤‡ä»½
    if backup:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = f"{base_dir}_backup_before_rename_{timestamp}"
        if os.path.exists(backup_dir):
            shutil.rmtree(backup_dir)
        shutil.copytree(base_dir, backup_dir)
        print(f"âœ“ å·²åˆ›å»ºå¤‡ä»½: {backup_dir}")
    
    # è¯»å–ç°æœ‰ç±»åˆ«åç§°
    class_file_path = os.path.join(base_dir, class_files[0])
    class_names = read_class_names(class_file_path)
    
    if not class_names:
        print("é”™è¯¯: æ— æ³•è¯»å–ç±»åˆ«åç§°")
        return False
    
    print(f"åŸå§‹ç±»åˆ«: {class_names}")
    
    # åº”ç”¨é‡å‘½å
    updated_class_names = class_names.copy()
    for old_name, new_name in class_renames.items():
        if old_name in updated_class_names:
            index = updated_class_names.index(old_name)
            updated_class_names[index] = new_name
            print(f"âœ“ é‡å‘½å: {old_name} -> {new_name}")
        else:
            print(f"è­¦å‘Š: ç±»åˆ« '{old_name}' ä¸å­˜åœ¨")
    
    # æ›´æ–°æ‰€æœ‰ç±»åˆ«æ–‡ä»¶
    for class_file in class_files:
        class_file_path = os.path.join(base_dir, class_file)
        write_class_names(class_file_path, updated_class_names)
        print(f"âœ“ å·²æ›´æ–°ç±»åˆ«æ–‡ä»¶: {class_file}")
    
    print(f"æ›´æ–°åç±»åˆ«: {updated_class_names}")
    print("é‡å‘½åæ“ä½œå®Œæˆ!")
    
    return True


def cleanup_backups(base_dir, keep_count=5, dry_run=False):
    """æ¸…ç†æ—§çš„å¤‡ä»½æ–‡ä»¶å¤¹"""
    import glob
    import re
    
    print(f"\nğŸ§¹ æ¸…ç†å¤‡ä»½æ–‡ä»¶å¤¹...")
    
    # æŸ¥æ‰¾æ‰€æœ‰å¤‡ä»½æ–‡ä»¶å¤¹
    backup_pattern = f"{base_dir}_backup_*"
    backup_dirs = glob.glob(backup_pattern)
    
    if not backup_dirs:
        print("æœªæ‰¾åˆ°ä»»ä½•å¤‡ä»½æ–‡ä»¶å¤¹")
        return
    
    # æŒ‰æ—¶é—´æˆ³æ’åºï¼ˆæå–æ—¶é—´æˆ³éƒ¨åˆ†ï¼‰
    def extract_timestamp(path):
        # åŒ¹é…æ—¶é—´æˆ³æ ¼å¼ YYYYMMDD_HHMMSS
        match = re.search(r'_(\d{8}_\d{6})$', path)
        return match.group(1) if match else '00000000_000000'
    
    backup_dirs.sort(key=extract_timestamp, reverse=True)
    
    print(f"æ‰¾åˆ° {len(backup_dirs)} ä¸ªå¤‡ä»½æ–‡ä»¶å¤¹:")
    for i, backup_dir in enumerate(backup_dirs):
        timestamp = extract_timestamp(backup_dir)
        size = get_folder_size(backup_dir)
        status = "ä¿ç•™" if i < keep_count else "åˆ é™¤"
        print(f"  {i+1}. {os.path.basename(backup_dir)} (æ—¶é—´: {timestamp}, å¤§å°: {size:.1f}MB) - {status}")
    
    # åˆ é™¤è¶…å‡ºä¿ç•™æ•°é‡çš„å¤‡ä»½
    to_delete = backup_dirs[keep_count:]
    
    if not to_delete:
        print(f"âœ“ æ‰€æœ‰å¤‡ä»½éƒ½åœ¨ä¿ç•™èŒƒå›´å†… (ä¿ç•™æœ€æ–° {keep_count} ä¸ª)")
        return
    
    if dry_run:
        print(f"\n[æ¼”ä¹ æ¨¡å¼] å°†è¦åˆ é™¤ {len(to_delete)} ä¸ªæ—§å¤‡ä»½:")
        for backup_dir in to_delete:
            print(f"  - {backup_dir}")
        print("ä½¿ç”¨ --execute å‚æ•°æ‰§è¡Œå®é™…åˆ é™¤")
        return
    
    print(f"\nå¼€å§‹åˆ é™¤ {len(to_delete)} ä¸ªæ—§å¤‡ä»½...")
    deleted_count = 0
    total_size_freed = 0
    
    for backup_dir in to_delete:
        try:
            size = get_folder_size(backup_dir)
            shutil.rmtree(backup_dir)
            print(f"âœ“ å·²åˆ é™¤: {backup_dir}")
            deleted_count += 1
            total_size_freed += size
        except Exception as e:
            print(f"âœ— åˆ é™¤å¤±è´¥: {backup_dir} - {e}")
    
    print(f"\næ¸…ç†å®Œæˆ:")
    print(f"åˆ é™¤äº† {deleted_count} ä¸ªå¤‡ä»½æ–‡ä»¶å¤¹")
    print(f"é‡Šæ”¾ç©ºé—´: {total_size_freed:.1f}MB")
    print(f"ä¿ç•™æœ€æ–° {len(backup_dirs) - deleted_count} ä¸ªå¤‡ä»½")


def show_dataset_info(base_dir):
    """æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯"""
    print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯åˆ†æ: {base_dir}")
    print("=" * 50)
    
    # åˆ†ææ•°æ®é›†
    class_usage, class_names = analyze_dataset_classes(base_dir)
    if class_usage is None:
        return
    
    # æ˜¾ç¤ºç±»åˆ«ä¿¡æ¯
    print(f"ğŸ“‹ ç±»åˆ«å®šä¹‰ (å…± {len(class_names)} ä¸ª):")
    for i, name in enumerate(class_names):
        usage_count = class_usage.get(i, 0)
        print(f"  {i}: {name} (ä½¿ç”¨ {usage_count} æ¬¡)")
    
    # æ˜¾ç¤ºä½¿ç”¨ç»Ÿè®¡
    print(f"\nğŸ“ˆ ç±»åˆ«ä½¿ç”¨ç»Ÿè®¡:")
    total_annotations = sum(class_usage.values())
    print(f"æ€»æ ‡æ³¨æ•°é‡: {total_annotations}")
    
    if class_usage:
        used_classes = len(class_usage)
        unused_classes = [i for i in range(len(class_names)) if i not in class_usage]
        
        print(f"å·²ä½¿ç”¨ç±»åˆ«: {used_classes}")
        if unused_classes:
            print(f"æœªä½¿ç”¨ç±»åˆ«: {unused_classes}")
            print(f"æœªä½¿ç”¨ç±»åˆ«åç§°: {[class_names[i] for i in unused_classes if i < len(class_names)]}")
        
        # æ˜¾ç¤ºä½¿ç”¨é¢‘ç‡æ’åº
        sorted_usage = sorted(class_usage.items(), key=lambda x: x[1], reverse=True)
        print(f"\nä½¿ç”¨é¢‘ç‡æ’åº:")
        for class_id, count in sorted_usage:
            class_name = class_names[class_id] if class_id < len(class_names) else f"æœªçŸ¥ç±»åˆ«{class_id}"
            percentage = count / total_annotations * 100
            print(f"  ç±»åˆ« {class_id} ({class_name}): {count} æ¬¡ ({percentage:.1f}%)")




def reindex_classes(base_dir, target_class_names, strict=False, backup=True, dry_run=True, require_same_set=False):
    """æ ¹æ®ç›®æ ‡ç±»åˆ«é¡ºåºé‡æ’æ•°æ®é›†ä¸­æ‰€æœ‰æ ‡ç­¾æ–‡ä»¶çš„ç±»åˆ«IDï¼Œå¹¶æ›´æ–°ç±»åˆ«æ–‡ä»¶ã€‚

    å‚æ•°:
    - base_dir: æ•°æ®é›†æ ¹ç›®å½•
    - target_class_names: ç›®æ ‡ç±»åˆ«é¡ºåº(list[str])ï¼Œå†™å…¥ç±»åˆ«æ–‡ä»¶å¹¶ä½œä¸ºæ–°IDæ˜ å°„ä¾æ®
    - strict: è‹¥ä¸ºTrueï¼Œé‡åˆ°æ—§ç±»åˆ«åœ¨ç›®æ ‡è¡¨ä¸­ä¸å­˜åœ¨åˆ™æŠ¥é”™ç»ˆæ­¢ï¼›å¦åˆ™è·³è¿‡è¯¥æ ‡æ³¨
    - backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½
    - dry_run: æ¼”ä¹ æ¨¡å¼ï¼Œä»…ç»Ÿè®¡ä¸é¢„è§ˆï¼Œä¸å®é™…æ”¹å†™
    """
    # è¯»å–å½“å‰ç±»åˆ«
    class_files = list_possible_class_files(base_dir)
    if not class_files:
        print("é”™è¯¯: æœªæ‰¾åˆ°ç±»åˆ«æ–‡ä»¶ï¼Œæ— æ³•è¿›è¡Œé‡æ’")
        return False
    current_class_file = os.path.join(base_dir, class_files[0])
    current_names = read_class_names(current_class_file)
    if not current_names:
        print("é”™è¯¯: æ— æ³•è¯»å–å½“å‰ç±»åˆ«åç§°")
        return False

    print("å½“å‰ç±»åˆ«é¡ºåº:", current_names)
    print("ç›®æ ‡ç±»åˆ«é¡ºåº:", target_class_names)

    if require_same_set:
        cur_set = set(current_names)
        tgt_set = set(target_class_names)
        if cur_set != tgt_set:
            only_in_cur = sorted(list(cur_set - tgt_set))
            only_in_tgt = sorted(list(tgt_set - cur_set))
            print("é”™è¯¯: ç±»åˆ«é›†åˆä¸ä¸€è‡´ï¼Œå·²å¯ç”¨ --require-same-setï¼š")
            if only_in_cur:
                print(f"  ä»…åœ¨å½“å‰é›†åˆä¸­å­˜åœ¨: {only_in_cur}")
            if only_in_tgt:
                print(f"  ä»…åœ¨ç›®æ ‡é›†åˆä¸­å­˜åœ¨: {only_in_tgt}")
            return False

    # æ„å»ºæ˜ å°„: æ—§class_id -> æ–°class_id
    name_to_new = {n: i for i, n in enumerate(target_class_names)}
    missing_old = [n for n in current_names if n not in name_to_new]
    if missing_old:
        msg = f"è­¦å‘Š: ä»¥ä¸‹æ—§ç±»åˆ«åœ¨ç›®æ ‡åˆ—è¡¨ä¸­ä¸å­˜åœ¨: {missing_old}"
        if strict:
            print("é”™è¯¯(ä¸¥æ ¼æ¨¡å¼):", msg)
            return False
        else:
            print(msg + "ï¼Œè¿™äº›ç±»åˆ«çš„æ ‡æ³¨å°†è¢«ä¸¢å¼ƒ")

    old_to_new = {}
    for old_id, name in enumerate(current_names):
        if name in name_to_new:
            old_to_new[old_id] = name_to_new[name]
        else:
            old_to_new[old_id] = None  # ä¸¢å¼ƒ

    # åˆ›å»ºå¤‡ä»½
    if backup and not dry_run:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = f"{base_dir}_backup_before_reindex_{timestamp}"
        if os.path.exists(backup_dir):
            shutil.rmtree(backup_dir)
        shutil.copytree(base_dir, backup_dir)
        print(f"âœ“ å·²åˆ›å»ºå¤‡ä»½: {backup_dir}")

    structure, _, _ = detect_yolo_structure(base_dir)
    label_dirs = yolo_label_dirs(base_dir, structure)

    updated_files = 0
    dropped_annotations = 0
    total_annotations = 0

    for labels_dir in label_dirs:
        for label_file in iter_label_files(labels_dir, structure):
            label_path = os.path.join(labels_dir, label_file)
            try:
                new_lines = []
                with open(label_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        s = line.strip()
                        if not s:
                            continue
                        parts = s.split()
                        try:
                            old_id = int(parts[0])
                        except Exception:
                            continue
                        total_annotations += 1
                        new_id = old_to_new.get(old_id, None)
                        if new_id is None:
                            dropped_annotations += 1
                            continue
                        parts[0] = str(new_id)
                        new_lines.append(' '.join(parts))

                if not dry_run:
                    with open(label_path, 'w', encoding='utf-8') as f:
                        for ln in new_lines:
                            f.write(ln + '\n')
                updated_files += 1
            except Exception as e:
                print(f"é”™è¯¯: æ— æ³•å¤„ç†æ ‡ç­¾æ–‡ä»¶ {label_path}: {e}")

    # æ›´æ–°ç±»åˆ«æ–‡ä»¶
    if not dry_run:
        for class_file in class_files:
            class_file_path = os.path.join(base_dir, class_file)
            write_class_names(class_file_path, target_class_names)
            print(f"âœ“ å·²æ›´æ–°ç±»åˆ«æ–‡ä»¶: {class_file}")

    print("\né‡æ’å®Œæˆ(é¢„è§ˆ)" if dry_run else "\né‡æ’å®Œæˆ")
    print(f"å¤„ç†çš„æ ‡ç­¾æ–‡ä»¶: {updated_files}")
    print(f"æ€»æ ‡æ³¨: {total_annotations}")
    if missing_old:
        print(f"ä¸¢å¼ƒçš„æ ‡æ³¨(å› ç±»åˆ«ç¼ºå¤±): {dropped_annotations}")
    return True


def main():
    parser = argparse.ArgumentParser(description="YOLOæ•°æ®é›†ç±»åˆ«ç®¡ç†å·¥å…·")
    parser.add_argument("--dataset_dir", "-d", required=True,
                       help="æ•°æ®é›†ç›®å½•è·¯å¾„")
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # ä¿¡æ¯å‘½ä»¤
    info_parser = subparsers.add_parser('info', help='æ˜¾ç¤ºæ•°æ®é›†ç±»åˆ«ä¿¡æ¯')
    
    # åˆ é™¤å‘½ä»¤
    delete_parser = subparsers.add_parser('delete', help='åˆ é™¤æŒ‡å®šç±»åˆ«')
    delete_parser.add_argument("--class_ids", "-c", nargs='+', type=int, required=True,
                              help="è¦åˆ é™¤çš„ç±»åˆ«IDåˆ—è¡¨")
    delete_parser.add_argument("--no-backup", action="store_true",
                              help="ä¸åˆ›å»ºå¤‡ä»½")
    
    # é‡å‘½åå‘½ä»¤
    rename_parser = subparsers.add_parser('rename', help='é‡å‘½åç±»åˆ«')
    rename_parser.add_argument("--renames", "-r", nargs='+', required=True,
                              help="é‡å‘½åæ˜ å°„ï¼Œæ ¼å¼: old_name1:new_name1 old_name2:new_name2")
    rename_parser.add_argument("--no-backup", action="store_true",
                              help="ä¸åˆ›å»ºå¤‡ä»½")
    
    # å¤‡ä»½æ¸…ç†å‘½ä»¤
    cleanup_parser = subparsers.add_parser('cleanup', help='æ¸…ç†æ—§çš„å¤‡ä»½æ–‡ä»¶å¤¹')
    cleanup_parser.add_argument("--keep", type=int, default=5,
                               help="ä¿ç•™æœ€æ–°çš„å¤‡ä»½æ•°é‡ (é»˜è®¤: 5)")
    cleanup_parser.add_argument("--dry-run", action="store_true",
                               help="æ¼”ä¹ æ¨¡å¼ï¼Œåªæ˜¾ç¤ºå°†è¦åˆ é™¤çš„å¤‡ä»½ï¼Œä¸æ‰§è¡Œå®é™…åˆ é™¤")
    cleanup_parser.add_argument("--execute", action="store_true",
                               help="æ‰§è¡Œå®é™…åˆ é™¤æ“ä½œ")

    # é‡æ’å‘½ä»¤
    reindex_parser = subparsers.add_parser('reindex', help='æ ¹æ®ç›®æ ‡ç±»åˆ«é¡ºåºé‡æ’æ‰€æœ‰æ ‡ç­¾ä¸­çš„ç±»åˆ«IDï¼Œå¹¶æ›´æ–°ç±»åˆ«æ–‡ä»¶')
    reindex_group = reindex_parser.add_mutually_exclusive_group(required=True)
    reindex_group.add_argument("--to-file", dest="to_file", help="ç›®æ ‡ç±»åˆ«æ–‡ä»¶è·¯å¾„ (classes.txt æˆ– *.yaml)ï¼Œå…¶é¡ºåºå°†ä½œä¸ºæ–°ç±»åˆ«é¡ºåº")
    reindex_group.add_argument("--to-classes", dest="to_classes", nargs='+', help="ç›´æ¥æä¾›ç›®æ ‡ç±»åˆ«é¡ºåºåˆ—è¡¨ï¼Œä¾‹å¦‚: --to-classes arco_0 arco_1 arco_2 ...")
    reindex_parser.add_argument("--strict", action="store_true", help="ä¸¥æ ¼æ¨¡å¼: æ—§ç±»åˆ«è‹¥ä¸åœ¨ç›®æ ‡åˆ—è¡¨ä¸­åˆ™æŠ¥é”™ç»ˆæ­¢")
    reindex_parser.add_argument("--require-same-set", action="store_true", help="å¼ºåˆ¶è¦æ±‚å½“å‰ä¸ç›®æ ‡ç±»åˆ«é›†åˆå®Œå…¨ä¸€è‡´ï¼Œå¦åˆ™ä¸­æ­¢")
    reindex_parser.add_argument("--no-backup", action="store_true", help="ä¸åˆ›å»ºå¤‡ä»½")
    reindex_parser.add_argument("--dry-run", action="store_true", help="æ¼”ä¹ æ¨¡å¼ï¼Œä»…é¢„è§ˆæ›´æ”¹ï¼Œä¸å†™å›ç£ç›˜ (é»˜è®¤: é¢„è§ˆ)")
    reindex_parser.add_argument("--execute", action="store_true", help="æ‰§è¡Œå®é™…é‡æ’(ä¸ --dry-run äº’æ–¥)")

    # delete é˜ˆå€¼æ‰©å±•å‚æ•°
    delete_parser.add_argument("--min-samples", type=int, help="åˆ é™¤æ ·æœ¬æ•°å°‘äºé˜ˆå€¼çš„ç±»åˆ«")
    delete_parser.add_argument("--min-percentage", type=float, help="åˆ é™¤å æ¯”å°‘äºé˜ˆå€¼(%)çš„ç±»åˆ«")
    delete_parser.add_argument("--dry-run", action="store_true", help="é¢„è§ˆåˆ é™¤ï¼Œä¸å†™å…¥ç£ç›˜")
    delete_parser.add_argument("--execute", action="store_true", help="å®é™…æ‰§è¡Œåˆ é™¤(ä¸ --dry-run äº’æ–¥ï¼Œé»˜è®¤é¢„è§ˆ)")
    delete_parser.add_argument("--yes", action="store_true", help="æ— éœ€ç¡®è®¤ç›´æ¥æ‰§è¡Œ")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # éªŒè¯æ•°æ®é›†ç›®å½•
    if not os.path.exists(args.dataset_dir):
        print(f"é”™è¯¯: æ•°æ®é›†ç›®å½• {args.dataset_dir} ä¸å­˜åœ¨")
        return
    
    if args.command == 'info':
        show_dataset_info(args.dataset_dir)
    
    elif args.command == 'delete':
        if args.dry_run and args.execute:
            print("é”™è¯¯: --dry-run ä¸ --execute ä¸èƒ½åŒæ—¶ä½¿ç”¨")
            return
        dry_run = args.dry_run or not args.execute
        backup = not args.no_backup
        delete_classes(
            args.dataset_dir,
            explicit_class_ids=args.class_ids,
            backup=backup,
            min_samples=args.min_samples,
            min_percentage=args.min_percentage,
            assume_yes=args.yes,
            dry_run=dry_run,
        )
    
    elif args.command == 'rename':
        backup = not args.no_backup
        
        # è§£æé‡å‘½åæ˜ å°„
        class_renames = {}
        for rename_pair in args.renames:
            try:
                old_name, new_name = rename_pair.split(':', 1)
                class_renames[old_name] = new_name
            except ValueError:
                print(f"é”™è¯¯: æ— æ•ˆçš„é‡å‘½åæ ¼å¼ '{rename_pair}'ï¼Œåº”ä¸º 'old_name:new_name'")
                return
        
        rename_classes(args.dataset_dir, class_renames, backup)
    
    elif args.command == 'cleanup':
        if args.dry_run and args.execute:
            print("é”™è¯¯: --dry-run å’Œ --execute ä¸èƒ½åŒæ—¶ä½¿ç”¨")
            return
        
        dry_run = args.dry_run or not args.execute
        cleanup_backups(args.dataset_dir, args.keep, dry_run)

    elif args.command == 'reindex':
        if args.dry_run and args.execute:
            print("é”™è¯¯: --dry-run å’Œ --execute ä¸èƒ½åŒæ—¶ä½¿ç”¨")
            return
        dry_run = args.dry_run or not args.execute
        backup = not args.no_backup

        # è¯»å–ç›®æ ‡ç±»åˆ«
        if args.to_file:
            if not os.path.exists(args.to_file):
                print(f"é”™è¯¯: ç›®æ ‡ç±»åˆ«æ–‡ä»¶ä¸å­˜åœ¨: {args.to_file}")
                return
            target_names = read_class_names(args.to_file)
        else:
            target_names = args.to_classes or []

        if not target_names:
            print("é”™è¯¯: ç›®æ ‡ç±»åˆ«åˆ—è¡¨ä¸ºç©º")
            return

        reindex_classes(
            args.dataset_dir,
            target_names,
            strict=args.strict,
            backup=backup,
            dry_run=dry_run,
            require_same_set=args.require_same_set,
        )

    # clean å­å‘½ä»¤å·²ç§»é™¤


if __name__ == "__main__":
    main()
