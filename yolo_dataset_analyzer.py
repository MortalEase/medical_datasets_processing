import os
from pathlib import Path
import argparse
import yaml
import random


def get_image_extensions():
    """è¿”å›æ”¯æŒçš„å›¾ç‰‡æ ¼å¼æ‰©å±•å"""
    return ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.webp']


def find_classes_file(dataset_dir):
    """æŸ¥æ‰¾classes.txtæ–‡ä»¶"""
    possible_paths = [
        os.path.join(dataset_dir, 'classes.txt'),
        os.path.join(dataset_dir, 'obj.names'),
        os.path.join(dataset_dir, 'names.txt'),
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return path
    return None


def find_data_yaml(dataset_dir):
    """æŸ¥æ‰¾data.yamlæ–‡ä»¶"""
    possible_paths = [
        os.path.join(dataset_dir, 'data.yaml'),
        os.path.join(dataset_dir, 'data.yml'),
        os.path.join(dataset_dir, 'dataset.yaml'),
        os.path.join(dataset_dir, 'dataset.yml'),
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return path
    return None


def load_class_names(dataset_dir):
    """åŠ è½½ç±»åˆ«åç§°"""
    # ä¼˜å…ˆæŸ¥æ‰¾data.yaml
    yaml_path = find_data_yaml(dataset_dir)
    if yaml_path:
        try:
            with open(yaml_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                if 'names' in data:
                    if isinstance(data['names'], list):
                        return {i: name for i, name in enumerate(data['names'])}
                    elif isinstance(data['names'], dict):
                        return data['names']
        except:
            pass
    
    # æŸ¥æ‰¾classes.txt
    classes_path = find_classes_file(dataset_dir)
    if classes_path:
        try:
            with open(classes_path, 'r', encoding='utf-8') as f:
                names = [line.strip() for line in f if line.strip()]
                return {i: name for i, name in enumerate(names)}
        except:
            pass
    
    return {}


def detect_dataset_structure(dataset_dir):
    """æ£€æµ‹æ•°æ®é›†ç»“æ„ç±»å‹"""
    images_dir = os.path.join(dataset_dir, 'images')
    labels_dir = os.path.join(dataset_dir, 'labels')
    
    # æ£€æŸ¥æ˜¯å¦æœ‰train/val/teståˆ†å±‚ç»“æ„
    train_images = os.path.join(dataset_dir, 'train', 'images')
    train_labels = os.path.join(dataset_dir, 'train', 'labels')
    val_images = os.path.join(dataset_dir, 'val', 'images')
    val_labels = os.path.join(dataset_dir, 'val', 'labels')
    test_images = os.path.join(dataset_dir, 'test', 'images')
    test_labels = os.path.join(dataset_dir, 'test', 'labels')
    
    if (os.path.exists(train_images) and os.path.exists(train_labels)) or \
       (os.path.exists(val_images) and os.path.exists(val_labels)) or \
       (os.path.exists(test_images) and os.path.exists(test_labels)):
        return 'hierarchical'  # åˆ†å±‚ç»“æ„
    elif os.path.exists(images_dir) and os.path.exists(labels_dir):
        return 'simple'  # ç®€å•ç»“æ„
    else:
        return 'unknown'


def get_dataset_paths(dataset_dir):
    """æ ¹æ®æ•°æ®é›†ç»“æ„è·å–æ‰€æœ‰imageså’Œlabelsè·¯å¾„"""
    structure = detect_dataset_structure(dataset_dir)
    paths = []
    
    if structure == 'hierarchical':
        # åˆ†å±‚ç»“æ„
        for split in ['train', 'val', 'test']:
            images_dir = os.path.join(dataset_dir, split, 'images')
            labels_dir = os.path.join(dataset_dir, split, 'labels')
            if os.path.exists(images_dir) and os.path.exists(labels_dir):
                paths.append((split, images_dir, labels_dir))
    elif structure == 'simple':
        # ç®€å•ç»“æ„
        images_dir = os.path.join(dataset_dir, 'images')
        labels_dir = os.path.join(dataset_dir, 'labels')
        paths.append(('dataset', images_dir, labels_dir))
    
    return structure, paths


def check_yolo_dataset(img_dir, label_dir, img_exts=None):
    """
    æ£€æŸ¥YOLOæ•°æ®é›†å›¾ç‰‡ä¸æ ‡æ³¨çš„å¯¹åº”å…³ç³»
    """
    if img_exts is None:
        img_exts = get_image_extensions()
    
    # è·å–æ–‡ä»¶åé›†åˆï¼ˆä¸å«æ‰©å±•åï¼‰
    img_stems = {Path(f).stem for f in os.listdir(img_dir)
                 if Path(f).suffix.lower() in img_exts}

    label_stems = {Path(f).stem for f in os.listdir(label_dir)
                   if Path(f).suffix.lower() == '.txt'}

    # è®¡ç®—å·®å¼‚é›†åˆ
    missing_labels = img_stems - label_stems
    redundant_labels = label_stems - img_stems

    # ç”Ÿæˆå®Œæ•´æ–‡ä»¶ååˆ—è¡¨
    missing_files = []
    for stem in missing_labels:
        for ext in img_exts:
            f = Path(img_dir) / (stem + ext)
            if f.exists():
                missing_files.append(str(f))
                break

    redundant_files = [str(Path(label_dir) / (stem + '.txt'))
                       for stem in redundant_labels]

    return missing_files, redundant_files


def generate_report(split_name, missing, redundant):
    """ç”Ÿæˆæ£€æŸ¥æŠ¥å‘Š"""
    print(f"\n{'=' * 20} {split_name} æ£€æŸ¥æŠ¥å‘Š {'=' * 20}")
    print(f"ç¼ºå¤±æ ‡æ³¨æ–‡ä»¶: {len(missing)} ä¸ª")
    print(f"å†—ä½™æ ‡æ³¨æ–‡ä»¶: {len(redundant)} ä¸ª")

    if missing:
        print("\n[ ç¼ºå¤±æ ‡æ³¨çš„å›¾ç‰‡ ]")
        for f in missing[:5]:  # æœ€å¤šæ˜¾ç¤ºå‰5ä¸ª
            print(f"  ! {os.path.basename(f)}")
        if len(missing) > 5: 
            print(f"  ...ï¼ˆè¿˜æœ‰{len(missing)-5}ä¸ªï¼‰")

    if redundant:
        print("\n[ å†—ä½™çš„æ ‡æ³¨æ–‡ä»¶ ]")
        for f in redundant[:5]:
            print(f"  x {os.path.basename(f)}")
        if len(redundant) > 5: 
            print(f"  ...ï¼ˆè¿˜æœ‰{len(redundant)-5}ä¸ªï¼‰")


def analyze_annotation_statistics(img_dir, label_dir, split_name="", class_names=None):
    """åˆ†ææ ‡æ³¨ç»Ÿè®¡ä¿¡æ¯"""
    img_exts = get_image_extensions()
    total_images = 0
    labeled_images = 0
    total_boxes = 0
    class_counts = {}
    box_counts_per_image = []
    
    for f in os.listdir(img_dir):
        if Path(f).suffix.lower() in img_exts:
            total_images += 1
            label_path = Path(label_dir) / (Path(f).stem + '.txt')
            
            if label_path.exists():
                labeled_images += 1
                boxes_in_image = 0
                
                try:
                    with open(label_path, 'r') as file:
                        for line in file:
                            parts = line.strip().split()
                            if len(parts) >= 5:
                                class_id = int(float(parts[0]))
                                boxes_in_image += 1
                                total_boxes += 1
                                class_counts[class_id] = class_counts.get(class_id, 0) + 1
                except:
                    continue
                
                box_counts_per_image.append(boxes_in_image)
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    prefix = f"{split_name} " if split_name else ""
    print(f"\n{'='*30} {prefix}æ ‡æ³¨ç»Ÿè®¡åˆ†æ {'='*30}")
    print(f"ğŸ“Š å›¾ç‰‡æ€»æ•°: {total_images}")
    print(f"ğŸ“Š æœ‰æ ‡æ³¨å›¾ç‰‡æ•°: {labeled_images}")
    print(f"ğŸ“Š èƒŒæ™¯å›¾ç‰‡æ•°: {total_images - labeled_images}")
    print(f"ğŸ“Š æ ‡æ³¨æ¡†æ€»æ•°: {total_boxes}")
    if labeled_images > 0:
        print(f"ğŸ“Š å¹³å‡æ¯å¼ æœ‰æ ‡æ³¨å›¾ç‰‡çš„æ ‡æ³¨æ¡†æ•°: {total_boxes/labeled_images:.2f}")
    
    if box_counts_per_image:
        print(f"ğŸ“Š å•å¼ å›¾ç‰‡æœ€å¤šæ ‡æ³¨æ¡†æ•°: {max(box_counts_per_image)}")
        print(f"ğŸ“Š å•å¼ å›¾ç‰‡æœ€å°‘æ ‡æ³¨æ¡†æ•°: {min(box_counts_per_image)}")
    
    if class_counts:
        print(f"\nğŸ“ˆ å„ç±»åˆ«æ ‡æ³¨æ¡†åˆ†å¸ƒ:")
        for class_id in sorted(class_counts.keys()):
            count = class_counts[class_id]
            percentage = (count / total_boxes) * 100 if total_boxes > 0 else 0
            class_name = class_names.get(class_id, f"Class_{class_id}") if class_names else f"Class_{class_id}"
            print(f"   ç±»åˆ« {class_id} ({class_name}): {count} ä¸ª ({percentage:.1f}%)")
    
    return total_images, labeled_images, total_boxes, class_counts


def analyze_dataset(dataset_dir, show_stats=False):
    """åˆ†ææ•´ä¸ªæ•°æ®é›†"""
    print(f"ğŸ” å¼€å§‹åˆ†ææ•°æ®é›†: {dataset_dir}")
    
    # æ£€æµ‹æ•°æ®é›†ç»“æ„
    structure, paths = get_dataset_paths(dataset_dir)
    
    if not paths:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„YOLOæ•°æ®é›†ç»“æ„")
        print("æ”¯æŒçš„ç»“æ„:")
        print("  1. ç®€å•ç»“æ„: dataset/images/ + dataset/labels/")
        print("  2. åˆ†å±‚ç»“æ„: dataset/train/images/ + dataset/train/labels/ ç­‰")
        return
    
    # åŠ è½½ç±»åˆ«åç§°
    class_names = load_class_names(dataset_dir)
    
    print(f"ğŸ“ æ£€æµ‹åˆ°æ•°æ®é›†ç»“æ„: {'åˆ†å±‚ç»“æ„' if structure == 'hierarchical' else 'ç®€å•ç»“æ„'}")
    if class_names:
        print(f"ğŸ“‹ åŠ è½½äº† {len(class_names)} ä¸ªç±»åˆ«åç§°")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°ç±»åˆ«åç§°æ–‡ä»¶ (classes.txt æˆ– data.yaml)")
    
    # åˆ†ææ¯ä¸ªæ•°æ®é›†åˆ†å‰²
    total_missing = 0
    total_redundant = 0
    all_stats = {}
    
    for split_name, img_dir, label_dir in paths:
        # æ£€æŸ¥å¯¹åº”å…³ç³»
        missing, redundant = check_yolo_dataset(img_dir, label_dir)
        generate_report(split_name, missing, redundant)
        
        total_missing += len(missing)
        total_redundant += len(redundant)
        
        # ç»Ÿè®¡åˆ†æ
        if show_stats:
            stats = analyze_annotation_statistics(img_dir, label_dir, split_name, class_names)
            all_stats[split_name] = stats
    
    # æ€»ä½“æ‘˜è¦
    print(f"\n{'='*30} æ€»ä½“æ‘˜è¦ {'='*30}")
    print(f"ğŸ“Š æ•°æ®é›†åˆ†å‰²æ•°: {len(paths)}")
    print(f"âš ï¸  æ€»ç¼ºå¤±æ ‡æ³¨: {total_missing}")
    print(f"âš ï¸  æ€»å†—ä½™æ ‡æ³¨: {total_redundant}")
    
    if show_stats and all_stats:
        total_images = sum(stats[0] for stats in all_stats.values())
        total_labeled = sum(stats[1] for stats in all_stats.values())
        total_boxes = sum(stats[2] for stats in all_stats.values())
        
        print(f"ğŸ“Š æ€»å›¾ç‰‡æ•°: {total_images}")
        print(f"ğŸ“Š æ€»æœ‰æ ‡æ³¨å›¾ç‰‡æ•°: {total_labeled}")
        print(f"ğŸ“Š æ€»æ ‡æ³¨æ¡†æ•°: {total_boxes}")
        
        # åˆå¹¶ç±»åˆ«ç»Ÿè®¡
        combined_classes = {}
        for stats in all_stats.values():
            for class_id, count in stats[3].items():
                combined_classes[class_id] = combined_classes.get(class_id, 0) + count
        
        if combined_classes:
            print(f"\nğŸ“ˆ æ•´ä½“ç±»åˆ«åˆ†å¸ƒ:")
            for class_id in sorted(combined_classes.keys()):
                count = combined_classes[class_id]
                percentage = (count / total_boxes) * 100 if total_boxes > 0 else 0
                class_name = class_names.get(class_id, f"Class_{class_id}") if class_names else f"Class_{class_id}"
                print(f"   ç±»åˆ« {class_id} ({class_name}): {count} ä¸ª ({percentage:.1f}%)")


def main():
    parser = argparse.ArgumentParser(description="YOLOæ•°æ®é›†åˆ†æå·¥å…· - æ”¯æŒå¤šç§æ•°æ®é›†ç»“æ„")
    parser.add_argument('--dataset_dir', '-d', required=True, 
                       help='æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--stats', '-s', action='store_true', 
                       help='æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.dataset_dir):
        print(f"âŒ é”™è¯¯: æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {args.dataset_dir}")
        return
    
    analyze_dataset(args.dataset_dir, args.stats)


if __name__ == "__main__":
    main()